{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training notebook for SMART 3.0\n",
    "\n",
    "This notebook is written to replicate the training of SMART 3.0 model with Edited-HSQC data.  \n",
    "It is simplified just for the replicating the model.\n",
    "\n",
    "System Environment\n",
    "\n",
    "tensorflow == 2.3.0 (with CUDA 10.1)  \n",
    "keras == 2.4.0  \n",
    "numpy == 1.18.5  \n",
    "json == 2.0.9  \n",
    "requests == 2.22.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loding Package\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "K = tf.keras.backend\n",
    "\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from requests import get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "Two kind of data are essential for training the model.  \n",
    "Dataset contains HSQC data, finger prints, molecular weights, class, and glycoside information.  \n",
    "Index file contains the classificaion ontology from NPClassifier.  \n",
    "(https://chemrxiv.org/articles/preprint/NPClassifier_A_Deep_Neural_Network-Based_Structural_Classification_Tool_for_Natural_Products/12885494/1)\n",
    "\n",
    "dataset size == 210 MB  \n",
    "index_data == 56 KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dataset = 'https://www.dropbox.com/s/fa5if10971cm6sz/Training_set_1.npy?dl=1'\n",
    "url_index = 'https://www.dropbox.com/s/faqydz94mo2xy4d/index_v1.json?dl=1'\n",
    "r = get(url_dataset, allow_redirects=True)\n",
    "open('dataset.npy', 'wb').write(r.content)\n",
    "r = get(url_index, allow_redirects=True)\n",
    "open('index_v1.json', 'wb').write(r.content)\n",
    "\n",
    "dataset = np.load('dataset.npy', allow_pickle=True) # dataframe, FP, Class_No, and Glycoside\n",
    "with open('index_v1.json','rb') as r:\n",
    "    index = json.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data split for training and validation (stratified data split based on the compound classes)\n",
    "train_set, validation_set = train_test_split(dataset, test_size = 0.2, random_state=42,stratify=dataset[:,3] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(data):\n",
    "    images = np.zeros((len(data),128,128,2),int)\n",
    "    fps = np.zeros((len(data),6144),int)\n",
    "    mws = np.zeros((len(data),1),int)\n",
    "    clss = np.zeros((len(data),1),int)\n",
    "    glys = np.zeros((len(data),1),int)\n",
    "    for i in range(len(data)):\n",
    "        image = np.array(data[i][0])\n",
    "        for l in range(len(image)):\n",
    "            a, b, t = image[l][0], image[l][1], image[l][2]\n",
    "            try:\n",
    "                if t>=0 and 0 <= a < 128 and 0 <= b < 128:\n",
    "                    images[i, a, scale-b-1,0] = 1\n",
    "                elif t<0 and 0 <= a < 128 and 0 <= b < 128:    \n",
    "                    images[i, a, scale-b-1,1] = 1\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        fp = data[i][1]\n",
    "        for bit in fp:\n",
    "            fps[i][bit] = 1\n",
    "        \n",
    "        mws[i] = data[i][2]\n",
    "        \n",
    "        clss[i] = data[i][3]\n",
    "        glys[i] = data[i][4]\n",
    "        \n",
    "    clss =  tf.keras.utils.to_categorical(clss,num_classes=len(index['Superclass']))\n",
    "    glys = tf.keras.utils.to_categorical(glys,num_classes=2)\n",
    "    \n",
    "    # This outline improved the performance by giving spatial information of peaks on the HSQC spectra\n",
    "    images[:,:,0,:] = 1\n",
    "    images[:,:,127,:] = 1\n",
    "    images[:,0,:,:] = 1\n",
    "    images[:,127,:,:] = 1\n",
    "    \n",
    "    output = [fps, mws, clss, glys]\n",
    "    return images, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_CE():\n",
    "    # Not all compound are fully classified by NPClassifier.\n",
    "    # So those unclassified compounds were labelled as '-1' and this unclassified compounds were masked and not utilized during the backprogation.\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true_masked = tf.boolean_mask(y_true, tf.reduce_any(tf.not_equal(y_true, -1), 1))\n",
    "        y_pred_masked = tf.boolean_mask(y_pred, tf.reduce_any(tf.not_equal(y_true, -1), 1))\n",
    "        loss = K.mean(K.categorical_crossentropy(y_true_masked, y_pred_masked))\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def build_model():\n",
    "    def residual_block(X,n):\n",
    "        X_shortcut = X\n",
    "        X = layers.BatchNormalization()(X)\n",
    "        X = layers.Activation('relu')(X)\n",
    "        X = layers.Conv2D(n, (3, 3), padding=\"same\")(X)\n",
    "        X = layers.BatchNormalization()(X)\n",
    "        X = layers.Activation('relu')(X)\n",
    "        X = layers.Conv2D(n, (3, 3), padding=\"same\")(X)\n",
    "        X = layers.BatchNormalization()(X)\n",
    "        X = layers.Activation('relu')(X)\n",
    "        X = layers.Conv2D(n, (3, 3), padding=\"same\")(X)\n",
    "        X = layers.BatchNormalization()(X)\n",
    "        X = layers.Activation('relu')(X)\n",
    "        X = layers.Concatenate()([X,X_shortcut])\n",
    "        return X\n",
    "\n",
    "    Input_image = layers.Input(shape=(128,128,2))\n",
    "    X = layers.Conv2D(64, (3, 3), padding=\"same\")(Input_image)\n",
    "    X = layers.BatchNormalization()(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "    X = layers.Conv2D(64, (3, 3), padding=\"same\")(X)\n",
    "    X = layers.BatchNormalization()(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "    X = layers.MaxPooling2D((2, 2))(X)\n",
    "    X = residual_block(X,128)\n",
    "    X = layers.MaxPooling2D((2, 2))(X)\n",
    "    X = residual_block(X,256)\n",
    "    X = layers.MaxPooling2D((2, 2))(X)\n",
    "    X = residual_block(X,512)\n",
    "    X = layers.MaxPooling2D((2, 2))(X)\n",
    "    X = residual_block(X,1024)\n",
    "    X = layers.MaxPooling2D((2, 2))(X)\n",
    "    X = residual_block(X,2048)\n",
    "    X = layers.GlobalMaxPooling2D()(X)\n",
    "    X = layers.Dropout(0.2)(X)\n",
    "\n",
    "\n",
    "    FP_output = layers.Dense(6144, activation = 'sigmoid', name = 'FP')(X)\n",
    "    MW_output = layers.Dense(1, name = 'MW')(X)\n",
    "    output1 = layers.Dense(len(index['Superclass']), activation = 'softmax', name = 'class')(X)\n",
    "    output2 = layers.Dense(2, activation = 'softmax',name='glycoside')(X)\n",
    "\n",
    "    model = keras.Model(inputs = Input_image, outputs = [FP_output, MW_output, output1, output2])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=0.00001),loss=['binary_crossentropy','MAPE',masked_CE(),masked_CE()],loss_weights = [10000,1,1,1],metrics={'FP':'cosine_proximity','MW':'MAPE','class':'accuracy','glycoside':'accuracy'})\n",
    "    return model\n",
    "\n",
    "#Buiding model.\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the tensor\n",
    "train_x , train_y = data_preparation(train_set)\n",
    "val_x, val_y = data_preparation(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainig the model.\n",
    "model.fit(x=train_x,y=train_y,\n",
    "        validation_data=(val_x,val_y),batch_size=16, epochs=30,\n",
    "        use_multiprocessing=False, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
